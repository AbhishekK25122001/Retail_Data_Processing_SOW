{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_file_path = \"abfss://bronze@storagedb2nd.dfs.core.windows.net/logs/processing_log_bronze.log\"\n",
    "logger = logging.getLogger(\"RawDataPipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class ADLSHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            from pyspark.dbutils import DBUtils\n",
    "            dbutils = DBUtils(spark)\n",
    "            message = self.format(record)\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            full_message = f\"[{timestamp}] {message}\\n\"\n",
    "\n",
    "            try:\n",
    "                existing_logs = dbutils.fs.head(log_file_path, 1048576)\n",
    "            except Exception:\n",
    "                existing_logs = \"\"\n",
    "\n",
    "            updated_logs = existing_logs + full_message\n",
    "            dbutils.fs.put(log_file_path, updated_logs, overwrite=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write log: {e}\")\n",
    "\n",
    "log_handler = ADLSHandler()\n",
    "formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(formatter)\n",
    "\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(log_handler)\n",
    "\n",
    "logger.info(\"Logging initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #importing necessary packages\n",
    "    from pyspark.sql import*\n",
    "    from pyspark.sql.functions import*\n",
    "    import logging\n",
    "    import os\n",
    "    logger.info(\"Library imported successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 1: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89ab589-ca29-446b-8f03-c7f2c081e2cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "try:\n",
    "#Defining path\n",
    "    raw_path=\"abfss://rawsource@storagedb1st.dfs.core.windows.net/\"\n",
    "    bronze_path=\"abfss://bronze@storagedb2nd.dfs.core.windows.net/\"\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 4: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b406fe-af4d-45ef-8e76-6ef5ab8394b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.conf.set(\n",
    "        \"fs.azure.account.key.storagedb1st.dfs.core.windows.net\",\n",
    "        \"q+A9juaNLZgNsUisLKyJaYgRJ1DYNPH3K9cWTJV4JtCOfW2ZqQMH7h1nuQxHRd1T7TA1d/B8P9+2+ASt4LGsVw==\")\n",
    "\n",
    "    spark.conf.set(\n",
    "        \"fs.azure.account.key.storagedb2nd.dfs.core.windows.net\",\n",
    "        \"z9DPz4aRkQZ0cNa0RjKavadFnTQWd2YpklA8ar0G3W9TWkAM+xAr3PkBJRl6hDRrWPk3iHQnu2bo+AStKTow2g==\")\n",
    "    logger.info(\"Acess key verified successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 3: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66637543-afa8-4fbc-8f2d-5ef72f5ab97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# transforming sales.csv to Bronze sales.parquet\n",
    "\n",
    "try:\n",
    "    try:\n",
    "        #changing delivery data from csv to parquet\n",
    "        sales_df = spark.read.format('csv').option(\"header\", \"True\").option(\"inferSchema\", \"true\").load(f\"{raw_path}sales.csv\")\n",
    "        sales_bronze = sales_df.withColumn(\"date_of_ingestion\", current_timestamp()).withColumn('source_file',lit(\"sales.csv\"))\n",
    "        sales_bronze.write.mode(\"overwrite\").parquet(f\"{bronze_path}sales_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 5: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print('Bronze layer processing completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110e8678-a784-48fd-8f5e-291c78c55825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# transforming sales.csv to Bronze product.parquet\n",
    "\n",
    "try:\n",
    "    try:\n",
    "        #changing driver data from csv to parquet\n",
    "        product_df = spark.read.format('csv').option(\"header\", \"True\").option(\"inferSchema\", \"true\").load(f\"{raw_path}product.csv\")\n",
    "        product_bronze = product_df.withColumn(\"date_of_ingestion\", current_timestamp()).withColumn('source_file',lit(\"product.csv\"))\n",
    "        product_bronze.write.mode(\"overwrite\").parquet(f\"{bronze_path}product_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 6: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print('Bronze layer processing completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159dc8c9-7053-4e58-8ce0-8c64ed3bcf74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "    #transforming customer.csv to bronze customer.parquet\n",
    "\n",
    "try:\n",
    "    try:\n",
    "        #changing route data from csv to parquet\n",
    "        customer_df = spark.read.format('csv').option(\"header\", \"True\").option(\"inferSchema\", \"True\").load(f\"{raw_path}customer.csv\")\n",
    "        customer_bronze = customer_df.withColumn(\"date_of_ingestion\", current_timestamp()).withColumn('source_file',lit(\"customer.csv\"))\n",
    "        customer_bronze.write.mode(\"overwrite\").parquet(f\"{bronze_path}customer_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 7: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print('Bronze layer processing completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06744b5-528f-47e7-8c20-6c612d9e1856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "# transforming store.csv to Bronze store.parquet\n",
    "\n",
    "try:\n",
    "    try:\n",
    "        #changing vehicle data from csv to parquet\n",
    "        store_df=spark.read.format('csv').option(\"header\", \"True\").option(\"inferSchema\", \"True\").load(f\"{raw_path}store.csv\")\n",
    "        store_bronze = store_df.withColumn(\"date_of_ingestion\", current_timestamp()).withColumn('source_file',lit(\"store.csv\"))\n",
    "        store_bronze.write.mode(\"overwrite\").parquet(f\"{bronze_path}store_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 8: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "print('Bronze layer processing completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97aa1d53-0212-4aab-bc34-6483df6293ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_DataHandling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
